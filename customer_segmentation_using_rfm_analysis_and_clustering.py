# -*- coding: utf-8 -*-
"""Customer Segmentation Using RFM Analysis and Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SasDQrsJ8JtPrP-svqoaSP8m2JJHloDW
"""

# Title Of Project :Customer Segmentation Using RFM Analysis and Clustering
# URL for Dataset : https://archive.ics.uci.edu/dataset/352/online+retail

"""# Problem Statement:
Developed and implemented an RFM (Recency, Frequency, Monetary) model to segment customers for a UK-based online retail company, leading to more targeted marketing strategies. This approach identified key customer segments, boosting sales and enhancing client satisfaction.

"""

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""# Basic Data Understanding"""

data = pd.read_excel(r"/content/drive/MyDrive/csv_files/Online Retail.xlsx")

data.head()

# shape of data
data.shape

data.info()

data.describe()

"""# Handling Missing Value"""

# Data Cleaning

data.isnull().sum()

data[data["CustomerID"].isnull()]

misisng_percentage = data["Description"].isnull().mean()*100
print(f"Missing Description Percentage :{misisng_percentage:.2f}%")

# There are 0.27 missing value in "Description" column it's ok to drop the missing column
data.dropna(subset=["Description"], inplace=True)

# column "CustomerID"

missing_percentage = data['CustomerID'].isnull().mean() * 100
print(f"Missing CustomerID Percentage: {missing_percentage:.2f}%")

# Deleting rows with missing CustomerID values is often necessary to maintain data quality and integrity.
# Since CustomerID is a critical identifier for customers, missing values in this column can lead to inaccurate analysis and modeling.
# Removing these rows helps ensure that our results are based on complete and reliable data, reducing potential biases and improving the robustness of our insights.

data = data.dropna(subset=['CustomerID'])

data.isnull().sum()

data.shape

"""data cleaning done !"""

data[data.duplicated()] # data is not duplicated()

# there are 5225 duplicate values and aslo confirm .

data[(data["InvoiceNo"]==536409)&(data["StockCode"]==21866)&(data["Description"]=="UNION JACK FLAG LUGGAGE TAG")]

data.drop_duplicates(keep='first',inplace=True)

# chech again
data.duplicated().sum()

"""# Basic EDA"""

data.columns

# Top 10 Best-Selling Products by Description
data["Description"].value_counts().head(10)

plt.figure(figsize=(12,6))
sns.barplot(
    x=data["Description"].value_counts().head(10).values,
    y=data["Description"].value_counts().head(10).index,
    palette="coolwarm"
)
plt.xlabel("Count")
plt.title("Top 10 Product")

"""Observation:
The most popular product is the "WHITE HANGING HEART T-LIGHT HOLDER" with 2,058 sales, followed closely by the "REGENCY CAKESTAND 3 TIER" with 1,894 sales. This suggests that home decor items are highly sought after by customers.
"""

# Top 10 Best-Selling Products by Stock Code

data["StockCode"].value_counts().head(10)

plt.figure(figsize=(12,6))
sns.barplot(
    x = data["StockCode"].value_counts().head(10).index,
    y = data["StockCode"].value_counts().head(10).values,
    palette = "viridis"
)
plt.ylabel("Count")
plt.title("Top 10 Best-Selling Products by Stock Code")

"""Observation:

The stock code 85123A is the best-selling item, with 2,065 sales. There is a close correlation between the top-selling descriptions and their stock codes.
"""

# Count of Unique Countries with Sales
data["Country"].nunique()

# Top 10 Countries by Number of Sales
data["Country"].value_counts().head(10)

plt.figure(figsize= (12,6))
sns.countplot(
    data=data,
    x="Country",
    palette="viridis"
)
plt.xticks(rotation=90)
plt.title("Top 10 Countries by Number of Sales")

"""Observation:

The United Kingdom overwhelmingly leads in sales with 356,728 transactions, followed by Germany (9,480) and France (8,475). The UK market is the company's primary revenue driver.
"""

data["CustomerID"].dtype

# we need to changr the data type of "CustomerID"
data["CustomerID"] = data["CustomerID"].astype("str")

# Top 10 Customers by Number of Purchases
data["CustomerID"].value_counts().head(10)

plt.figure(figsize=(12,6))
sns.barplot(
    x=data["CustomerID"].value_counts().head(10).values,
    y=data["CustomerID"].value_counts().head(10).index,
    palette="viridis"
)
plt.xlabel("Count")
plt.title("Top 10 Customers by Sales")

"""Observation:
Customer ID 17841 made the highest number of purchases, with 7,812 transactions. This indicates a small number of customers contributing significantly to sales.
"""

# Top 10 Countries by Number of Unique Customers
count_cc = data.groupby("Country")["CustomerID"].count().sort_values(ascending = False).head(10)
count_cc

plt.figure(figsize=(12,6))
sns.barplot(
    x=count_cc.values,
    y=count_cc.index,
    palette="viridis"
)
plt.xlabel("Count")
plt.title("count of customer based on country")

"""Observation:

The distribution of unique customers by country mirrors the sales data, with the UK again leading significantly. This reinforces the UK market's dominance.
"""

# basic EDA Done

"""# Feature Engineering"""

data["InvoiceDate"] = pd.to_datetime(data["InvoiceDate"])

data["InvoiceDate"].head()

# split the "InvoiceDate" columns into "Day","Month","Year"
data["Day"] = data["InvoiceDate"].dt.day
data["Month"] = data["InvoiceDate"].dt.month
data["Year"] = data["InvoiceDate"].dt.year

# let's split time
data['Hour'] = data['InvoiceDate'].dt.hour
data['Minute'] = data['InvoiceDate'].dt.minute
data['Second'] = data['InvoiceDate'].dt.second

data["TotalAmount"] = data["Quantity"] * data["UnitPrice"]

# Update dataFrame after feature Engineering
data.head()



"""## Trend Over Time"""

invoice_counts = data['Hour'].value_counts().sort_index()

plt.figure(figsize=(8, 5))
plt.plot(invoice_counts.index, invoice_counts.values, marker='o')
plt.title('Number of Invoices per Hour')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Invoices')
plt.xticks(range(0, 24))  # Set x-axis to cover all 24 hours
plt.grid(True)
plt.show()

"""Observation:

The highest invoice volume occurs between 11 AM and 12 PM, indicating peak business activity during this time.

Daily Invoice Pattern: Invoices increase steadily from morning, peak at midday, and then decrease towards the evening, reflecting a consistent daily processing rhythm.
"""

data['MonthName'] = data["InvoiceDate"].dt.month_name()
Monthly_invoice = data["MonthName"].value_counts().sort_index()

month_order = ["January", "February", "March", "April", "May", "June",
               "July", "August", "September", "October", "November", "December"]

plt.figure(figsize=(12,8))
sns.lineplot(
    x=Monthly_invoice.index,
    y=Monthly_invoice.values,
    marker="o",color="red"
)
plt.xticks(ticks=range(len(month_order)), labels=month_order)
plt.xlabel("Month")
plt.ylabel("Number of Invoices")
plt.title("Monthly Invoice Trend")
plt.grid()
plt.show()

"""Observation:

Invoices fluctuate throughout the year, with a peak in October.
April and December see the lowest invoice volumes, indicating slower business activity.
Invoice volumes generally rise from January to October, then sharply decline towards the end of the year.

# Model Building

## RFM Analysis
"""

# New Attribute: Frequency - The number of purchases made by each customer.

rfm_f = data.groupby("CustomerID")["InvoiceNo"].count().reset_index().rename(columns = {'InvoiceNo':'Frequency'})
rfm_f.head()

# New Attribute: Recency - The difference between the latest order date in the dataset and the customer's most recent order date.

max_date = max(data["InvoiceDate"])
print(f"Latest order date in the dataset: {max_date}")

rfm_r = data.groupby("CustomerID")["InvoiceDate"].apply(lambda x: (max_date - x.max()).days).reset_index()
rfm_r.rename(columns = {'InvoiceDate':'Recency'},inplace=True)
rfm_r.head()

rfm_m = data.groupby("CustomerID")["TotalAmount"].sum().reset_index()
rfm_m.rename(columns={'TotalAmount': 'Monetary'}, inplace=True)

rfm = pd.merge(rfm_m, rfm_r, on='CustomerID')
rfm = pd.merge(rfm, rfm_f, on='CustomerID')

rfm.shape

rfm.head()

columns = ['Monetary','Recency','Frequency']
sns.boxplot(
    data = rfm[columns],
    palette = "Set2"
)

"""### Outlier detection and  Removal in RFM"""

# New attribute : Monetary : total Amount spend by each customer

rfm_m = data.groupby('CustomerID')["TotalAmount"].sum().reset_index().rename(columns = {'TotalAmount':'Monetary'})
rfm_m.head()

"""Observation:
There are large amount of Outlier in "Monetary" columns and also there is less number of outlier present in frquency .
"""

# dtetct Outlier in "Moneatry" columns
sns.histplot(
    data = rfm,
    x="Monetary",
    kde=True
)

# detect the Outlier
Q1 = rfm["Monetary"].quantile(0.25)
Q3 = rfm['Monetary'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR
outliers_m = rfm[(rfm['Monetary'] < lower_limit) | (rfm['Monetary'] > upper_limit)]

print("Outlier in Monetary:")
print(f"There are Total {len(outliers_m)} in 'Monteary' columns")
print(f"Index List  outlier in 'Monteary' columns:{outliers_m.index}")
print(outliers_m)

# remove the Outlier from 'Monteary' columns
rfm = rfm.drop(outliers_m.index)

# let's do another column detect and remove
Q1 = rfm["Recency"].quantile(0.25)
Q3 = rfm['Recency'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR
outliers_r = rfm[(rfm['Recency'] < lower_limit) | (rfm['Recency'] > upper_limit)]

print("Outlier in Recency:")
print(f"There are Total {len(outliers_r)} in 'Recency' columns")
print(f"Index List  outlier in 'Recency' columns:{outliers_r.index}")
print(outliers_r)

# remove the Outlier from 'Rencey' columns
rfm = rfm.drop(outliers_r.index)

rfm

# detect the Outlier
Q1 = rfm["Frequency"].quantile(0.25)
Q3 = rfm['Frequency'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5*IQR
upper_limit = Q3 + 1.5*IQR
outliers_f = rfm[(rfm['Frequency'] < lower_limit) | (rfm['Frequency'] > upper_limit)]

print("Outlier in Frequency:")
print(f"There are Total {len(outliers_f)} in 'Frequency' columns")
print(f"Index List  outlier in 'Monteary' columns:{outliers_f.index}")
print(outliers_f)

# remove the Outlier from 'Rencey' columns
rfm = rfm.drop(outliers_f.index)

rfm

# shape of rfm dataset after removal outlier
rfm.shape

"""### Standardize the Data"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
rfm_sc = sc.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

rfm_sc

"""## K-means Clustering Algorithm"""

# import libaries required for KMeans algoaritum
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# lets find optimal n_cluster number of cluster for Kmeans aaalgoritum by using "Within Cluster SUm of Squared elbow method"
wcss = []
for i in range(2,11):
  km = KMeans(n_clusters=i,init="k-means++",n_init=10,random_state=42)
  km.fit(rfm_sc)
  wcss.append(km.inertia_)
  print(f"for n_cluster={i} wcss is {km.inertia_}")

# plot the wcss graph vs n_cluster graph
plt.figure(figsize=(12,8))
sns.lineplot(x=range(2,11),
             y=wcss,
             color="red",
             marker="o")
plt.xlabel("Number of Cluster")
plt.ylabel("WCSS")
plt.title("WCSS vs Number of Cluster")
plt.grid()
plt.show()

for i in range(2,10):
  km = KMeans(n_clusters=i,
              init='k-means++',
              n_init= 10,
              random_state = 42)
  clusters = km.fit_predict(rfm_sc)
  sil_score = silhouette_score(rfm_sc,clusters)
  print(f"for {i} clusters Silhouette Score: {sil_score}")

"""*Observation*  
Considering both WCSS and the Silhouette Score, 3 clusters appear to be the optimal choice for this dataset.

The clusters formed are not only well-separated but also have a significant reduction in WCSS, which balances cluster tightness with minimal overlap between clusters.
"""

kmeans = KMeans(n_clusters=3,
                init='k-means++',
                n_init = 10,
                random_state = 42)
rfm["Clusters"] = kmeans.fit_predict(rfm_sc)
silhouette_avg = silhouette_score(rfm_sc, rfm["Clusters"])
print(f'Silhouette KMeans Score: {silhouette_avg:.2f}')

rfm

# plot the scatter plot
plt.figure(figsize=(10,6))
sns.scatterplot(data=rfm,
            x="Recency",
            y="Frequency",
            hue= "Clusters",
            s=60,
            palette='viridis')

plt.xlabel("Recency")
plt.ylabel("Frequency")
plt.title("Scatter Plot of Recency vs Frequency")
plt.show()

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(rfm['Recency'], rfm['Frequency'], rfm['Monetary'], c=rfm['Clusters'], cmap='viridis')
ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetary')
plt.title('3D Segmentation')
plt.colorbar(scatter)
plt.show()

"""Observation:

 K-Means produced a moderate silhouette score, indicating reasonably well-defined clusters with some overlap.

## DBSCAN Algorithm
"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

dbscan = DBSCAN(eps=0.5,
                min_samples=3)
dbscan.fit(rfm_sc)
labels = dbscan.labels_

score = silhouette_score(rfm_sc,labels)
print(f"Silhouette DBSCAN Score: {score}")

set(lables)

plt.figure(figsize = (10,8))
sns.scatterplot(
    data = rfm,
    x = "Recency",
    y="Frequency",
    hue = labels,
    s=60,
    palette = "viridis"
)

"""Observation:

DBSCAN yielded a low silhouette score, suggesting poorly defined clusters with potential noise.

## Hierarchical Clustering
"""

from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
model.fit(rfm_sc)
clusters = model.labels_

clusters

plt.figure(figsize = (10,8))
sns.scatterplot(
    data = rfm,
    x = "Recency",
    y="Frequency",
    hue = clusters,
    s=60,
    palette = "viridis"
)

"""Observation:

Agglomerative Clustering achieved a silhouette score slightly lower than K-Means, indicating moderately well-defined clusters, but still less distinct than K-Means.
"""

score = silhouette_score(rfm_sc,clusters)
print(f"Silhouette Score for AgglomerativeClustering: {score}")

"""# BEST Algorithm should be K-Means

# Selected Model: K-Means Clustering
K-Means with **n=3** clusters was selected due to its superior silhouette score, indicating the most effective clustering of customer segments.
"""

Cluster_Df = rfm.groupby(["Clusters"]).agg({
    "Monetary": "mean",
    "Recency": "mean",
    "Frequency": "mean"
}).reset_index()

print(Cluster_Df)

"""1) **Cluster 0** :  Customers in this cluster have high spending, frequent purchases, and recent interactions, indicating they are highly engaged and valuable to the business called as **High-Value Frequent Buyers(Whales)**

2) **Cluster 1** : Customers in this cluster have moderate spending, frequency, and recency, suggesting they are regular but not exceptional in terms of engagement or expenditure called as **Average Customers**

3) **Cluster 2** : Customers in this cluster have low spending, infrequent purchases, and have not interacted recently, indicating they are less engaged and at risk of churning called as **At-Risk Customers** . They are at risk of churning and need re-engagement strategies.
"""

Cluster_Df["Clusters"] = Cluster_Df["Clusters"].map({
    0: "Whales",
    1: "Avg Customers",
    2: "At-Risk Customers",
})

Cluster_Df

rfm

rfm["Clusters"] = rfm["Clusters"].map({
    0: "Whales",
    1: "Avg Customers",
    2: "At-Risk Customers",
})

# let's do EDA to know distribution as well as percentage of cutomer time
Counts = pd.DataFrame(rfm["Clusters"].value_counts().reset_index())
print(Counts)

plt.figure(figsize=(10, 6))
colors = ["#66b3ff", "#99ff99", "#ff9999"]
plt.pie(
    x=rfm["Clusters"].value_counts().values,
    labels=rfm["Clusters"].value_counts().index,
    autopct='%1.2f%%',
    startangle=140,
    colors=colors,
    wedgeprops={'edgecolor': 'black'},
    textprops={'fontsize': 14}
)

plt.title('Distribution of Customer Clusters', fontsize=16)

"""Observation:
The RFM analysis and customer segmentation reveal that the majority of customers fall into the "Avg Customers" category, comprising over 50% of the total customer base. The detailed distribution is as follows:

**Avg Customers**: 1919 customers (52.89%)

**At-Risk Customers**: 931 customers (25.66%)

**High-Value Frequent Buyers(Whales)**: 778 customers (21.44%)

This suggests that while the majority of customers are average in terms of recency, frequency, and monetary value, a significant portion of customers are at risk, and a smaller but valuable segment, the "High-Value Frequent Buyers (Whales)," represents high-value customers.

In conclusion, the RFM analysis helped identify different types of customers, giving us a better understanding of their behavior. These insights can now be used to create more effective marketing strategies, improve customer relationships, and focus on valuable customers to boost business growth and profits.
"""

